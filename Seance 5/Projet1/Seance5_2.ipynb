{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formation : Computer Vision with Python\n",
    "## Séance 5 : Image classification with CNN\n",
    "### Auteur : Nennouche Mohamed\n",
    "### Date : 28/02/2022\n",
    "### Contenu du notebook :\n",
    "On va au cours de cette séance introduire l'utilisation de Tensorflow dans les problème de Computer Vision sur des problématiques de classification utilisant des CNN. Donc on fera dans ce notebook :  \n",
    "- Introduction à Tensorflow \n",
    "- Comment créer un modèle avec Tensorflow\n",
    "    - Méthode séquentielle\n",
    "    - Méthode fonctionnelle\n",
    "    - Méthode avec les classes\n",
    "- Comment entrainer un modèle\n",
    "- Comment évaluer un modèle\n",
    "- Comment bien préparer les données pour les traiter avec Tensorflow\n",
    "\n",
    "Pour se faire on va utiliser une partie du dataset [GTSRB](https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign) pour la classification de différents types de plaques de signalisation routière"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Input, Dense, MaxPool2D, BatchNormalization, GlobalAvgPool2D, Flatten\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# for callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Préparation des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Split entraînement - validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par créer une fonction qui permet de splitter les données d'entraînement en deux dossiers : train et val, le premier pour l'entrainement du modèle et le deuxième pour sa validation (durant l'entraînement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = './Train'\n",
    "path_to_save_train = './Train_val/train'\n",
    "path_to_save_val = './Train_val/val'\n",
    "utils.split_data(path_to_data, path_to_save_train, path_to_save_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Préparation des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_images = './Test2'\n",
    "path_to_csv = './Test.csv'\n",
    "utils.order_test_set(path_to_images, path_to_csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mise en place du modèle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Informations préliminaires"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser GlobalAvgPool2D au lieu de Flatten, permettant de gagner beaucoup de paramètres lors de l'entraînement. Voilà un exemple d'utilisation de GlobalAvgPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4, 5, 3)\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (2, 4, 5, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 1.8397477  -1.3505247  -1.5800402 ]\n",
      "   [ 0.63768935 -1.1999903   0.38274378]\n",
      "   [-1.0280094  -1.579934    1.624443  ]\n",
      "   [-0.9099453   0.20088933  1.7756923 ]\n",
      "   [-1.8025535  -0.91793525  1.7371365 ]]\n",
      "\n",
      "  [[-0.80055195  0.68446356  1.5627073 ]\n",
      "   [ 1.2695603  -0.3232048  -0.959329  ]\n",
      "   [-0.8166272  -1.9070863  -1.3277104 ]\n",
      "   [ 0.4981948   1.992311    1.6657081 ]\n",
      "   [ 0.4804979  -0.10094799 -0.90984946]]\n",
      "\n",
      "  [[ 1.8846945   1.3698827  -0.44374925]\n",
      "   [-0.82751834  1.2349504  -0.42769873]\n",
      "   [-0.38052356  0.9910699  -0.22697818]\n",
      "   [-1.5485541   0.68284845 -0.19728404]\n",
      "   [-0.34907854 -0.4985748   0.9208069 ]]\n",
      "\n",
      "  [[-1.48777    -0.18260342 -0.2571184 ]\n",
      "   [-1.5337881  -1.17411     1.2933922 ]\n",
      "   [ 0.46214715 -0.6052504  -0.18489166]\n",
      "   [ 1.1469971   0.42211697 -0.4337584 ]\n",
      "   [ 0.93902516 -0.3441186   0.20144875]]]\n",
      "\n",
      "\n",
      " [[[-1.1927199   1.805365    1.140721  ]\n",
      "   [ 0.22117287  0.22867215  0.18312322]\n",
      "   [ 0.6205142   0.46545762 -1.2988738 ]\n",
      "   [-0.8268708  -0.20306446  0.05548193]\n",
      "   [-1.0412841  -0.12450704  0.02125244]]\n",
      "\n",
      "  [[-0.27244207  0.6411664  -2.0906937 ]\n",
      "   [ 0.65987736  0.33668602 -0.6142673 ]\n",
      "   [-0.36018255 -0.31719545 -1.2482396 ]\n",
      "   [ 1.0237752   0.93149436  0.3646732 ]\n",
      "   [ 1.3897964   0.53254056 -1.6186475 ]]\n",
      "\n",
      "  [[ 0.01946623 -0.4582069  -0.54791313]\n",
      "   [-2.275456   -0.40804744  1.3674769 ]\n",
      "   [-1.5155319  -0.5994333   1.0036008 ]\n",
      "   [-2.1145873  -0.22788076  1.833478  ]\n",
      "   [-1.1829128   0.11036494 -1.6689028 ]]\n",
      "\n",
      "  [[ 1.153       0.33947027 -0.9315401 ]\n",
      "   [ 1.4504713   1.1301805  -0.4353132 ]\n",
      "   [ 1.033554   -0.5563433   2.9833813 ]\n",
      "   [-1.0577605   1.4653003   1.4838815 ]\n",
      "   [ 1.4741703  -0.30511093  0.41150257]]]], shape=(2, 4, 5, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.11631831 -0.13028741  0.21078357]\n",
      " [-0.13969752  0.23934543  0.0197091 ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streetsigns_model(nbr_classes) :\n",
    "    my_input = Input(shape=(60,60, 3)) # taille moyenne et les 3 canaux RGB\n",
    "    x= Conv2D(32, (3,3), activation='relu')(my_input)\n",
    "    x= Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x= MaxPool2D()(x)\n",
    "    x= BatchNormalization()(x)\n",
    "\n",
    "    x= Conv2D(128, (3,3), activation='relu')(x)\n",
    "    x= MaxPool2D()(x)\n",
    "    x= BatchNormalization()(x)\n",
    "\n",
    "    x= GlobalAvgPool2D()(x) # On moyenne la totalité de la matrice\n",
    "    #x = Flatten()(x)\n",
    "    x= Dense(64, activation='relu')(x)\n",
    "    x= Dense(nbr_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=my_input, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60, 60, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 58, 58, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 28, 28, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 13, 13, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 128)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 43)                2795      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105,067\n",
      "Trainable params: 104,683\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = streetsigns_model(43)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement des données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générateur de données "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour alimenter notre modèle pour l'entrainement et ensuite le test, il contiendra : \n",
    "- préprocessing de toutes les images\n",
    "- préparation des images pour l'entrainement \n",
    "- acheminement des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35288 images belonging to 43 classes.\n",
      "Found 3921 images belonging to 43 classes.\n",
      "Found 12630 images belonging to 43 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_path = './Train_val/train'\n",
    "val_data_path = './Train_val/val'\n",
    "test_data_path = './Test2'\n",
    "batch_size = 64\n",
    "\n",
    "train_generator, val_generator, test_generator= utils.create_generators(batch_size, train_data_path, val_data_path, test_data_path)\n",
    "\n",
    "nbr_classes = train_generator.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = streetsigns_model(nbr_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrainement et fitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Préparation des callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "path_to_save_model = './Models' # ou sauvegarder le modèle\n",
    "ckpt_saver = ModelCheckpoint(\n",
    "    path_to_save_model,\n",
    "    monitor='val_accuracy', # sur quoi on se base pour voir le meilleur\n",
    "    mode = 'max', # max de l'accuracy sur la validation\n",
    "    save_best_only = True,\n",
    "    save_freq='epoch', # ne voit qu'à la fin de l'époque\n",
    "    verbose=1\n",
    ") \n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10 # après 10 époques ca change pas on s'arrête\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Compilation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # on choisit categorical_crossentropy car dans les générateurs on a défini categorical comme class_mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition du nombre d'époques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 2.4154 - accuracy: 0.3227\n",
      "Epoch 1: val_accuracy improved from -inf to 0.18490, saving model to .\\Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 828s 1s/step - loss: 2.4154 - accuracy: 0.3227 - val_loss: 3.1183 - val_accuracy: 0.1849\n",
      "Epoch 2/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 1.1370 - accuracy: 0.6674\n",
      "Epoch 2: val_accuracy improved from 0.18490 to 0.60367, saving model to .\\Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 473s 857ms/step - loss: 1.1370 - accuracy: 0.6674 - val_loss: 1.2401 - val_accuracy: 0.6037\n",
      "Epoch 3/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.8832\n",
      "Epoch 3: val_accuracy improved from 0.60367 to 0.87835, saving model to .\\Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 479s 867ms/step - loss: 0.4568 - accuracy: 0.8832 - val_loss: 0.4293 - val_accuracy: 0.8783\n",
      "Epoch 4/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.2013 - accuracy: 0.9541\n",
      "Epoch 4: val_accuracy improved from 0.87835 to 0.93114, saving model to .\\Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 476s 862ms/step - loss: 0.2013 - accuracy: 0.9541 - val_loss: 0.2504 - val_accuracy: 0.9311\n",
      "Epoch 5/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9737\n",
      "Epoch 5: val_accuracy improved from 0.93114 to 0.94593, saving model to .\\Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 478s 865ms/step - loss: 0.1132 - accuracy: 0.9737 - val_loss: 0.1843 - val_accuracy: 0.9459\n",
      "Epoch 6/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9813\n",
      "Epoch 6: val_accuracy improved from 0.94593 to 0.95613, saving model to .\\Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 473s 856ms/step - loss: 0.0816 - accuracy: 0.9813 - val_loss: 0.1423 - val_accuracy: 0.9561\n",
      "Epoch 7/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9881\n",
      "Epoch 7: val_accuracy did not improve from 0.95613\n",
      "552/552 [==============================] - 480s 870ms/step - loss: 0.0544 - accuracy: 0.9881 - val_loss: 0.1600 - val_accuracy: 0.9495\n",
      "Epoch 8/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9890\n",
      "Epoch 8: val_accuracy improved from 0.95613 to 0.96072, saving model to .\\Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 478s 866ms/step - loss: 0.0473 - accuracy: 0.9890 - val_loss: 0.1209 - val_accuracy: 0.9607\n",
      "Epoch 9/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9901\n",
      "Epoch 9: val_accuracy did not improve from 0.96072\n",
      "552/552 [==============================] - 857s 2s/step - loss: 0.0384 - accuracy: 0.9901 - val_loss: 0.3777 - val_accuracy: 0.9039\n",
      "Epoch 10/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9928\n",
      "Epoch 10: val_accuracy improved from 0.96072 to 0.97118, saving model to .\\Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 469s 850ms/step - loss: 0.0310 - accuracy: 0.9928 - val_loss: 0.0994 - val_accuracy: 0.9712\n",
      "Epoch 11/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9932\n",
      "Epoch 11: val_accuracy did not improve from 0.97118\n",
      "552/552 [==============================] - 465s 842ms/step - loss: 0.0266 - accuracy: 0.9932 - val_loss: 0.1023 - val_accuracy: 0.9681\n",
      "Epoch 12/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9934\n",
      "Epoch 12: val_accuracy improved from 0.97118 to 0.98036, saving model to .\\Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 464s 841ms/step - loss: 0.0265 - accuracy: 0.9934 - val_loss: 0.0676 - val_accuracy: 0.9804\n",
      "Epoch 13/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9900\n",
      "Epoch 13: val_accuracy did not improve from 0.98036\n",
      "552/552 [==============================] - 450s 815ms/step - loss: 0.0339 - accuracy: 0.9900 - val_loss: 0.1493 - val_accuracy: 0.9541\n",
      "Epoch 14/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9953\n",
      "Epoch 14: val_accuracy improved from 0.98036 to 0.99107, saving model to .\\Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 455s 825ms/step - loss: 0.0183 - accuracy: 0.9953 - val_loss: 0.0340 - val_accuracy: 0.9911\n",
      "Epoch 15/15\n",
      "552/552 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9969\n",
      "Epoch 15: val_accuracy did not improve from 0.99107\n",
      "552/552 [==============================] - 468s 848ms/step - loss: 0.0132 - accuracy: 0.9969 - val_loss: 0.0520 - val_accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc04790730>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Le générateur contient autant les images que les labels\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = val_generator,\n",
    "    callbacks=[ckpt_saver, early_stop]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chargement et évaluation du modèle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 60, 60, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 58, 58, 32)        896       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 28, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 28, 28, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 26, 26, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 13, 13, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 13, 13, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 128)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 43)                2795      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105,067\n",
      "Trainable params: 104,683\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./Models')\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 [==============================] - 125s 633ms/step - loss: 0.3018 - accuracy: 0.9289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3017587661743164, 0.9288994669914246]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Comment améliorer le modèle ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour améliorer le modèle on a plusieurs possibilités : \n",
    "- Changer la taille du Batch\n",
    "- Augmenter (ou diminuer) le nombre d'époques\n",
    "- Changer l'architecture du modèle (changer les couches ou en ajouter ou diminuer)\n",
    "- Dans la création du générateur (dans la partie ImageDataGenerator) il y a un certain nombre de techniques pour de la data augmentation qu'on peut tenter.\n",
    "- On peut mettre en place plusieurs pré-processeurs pour les adapter, chacun à une partie du problème (train et pas validation et test par exemple) surtout dans le cas de l'augmentation des données (avec des shifts et des zoom) \n",
    "- On peut changer l'optimizer en utilisant opitmizer = tf.keras.optimizers.NomOptimizer() et on choisit d'après la documentation qu'on a\n",
    "- Ajouter et changer le learning rate et le momentum et l'ajouter à l'optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Test du modèle à part entière"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 149ms/step\n",
      "La classe est 0\n"
     ]
    }
   ],
   "source": [
    "img_path = \"./Test2/0/00403.png\"\n",
    "model = tf.keras.models.load_model('./Models')\n",
    "prediction = utils.predict_with_model(model, img_path)\n",
    "print(f'La classe est {prediction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "631af202ec7c40e141c49d63cac6c8cb664469fed6e4d5ddc6436b49edc60500"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
